{
    "componentChunkName": "component---src-pages-event-automation-async-api-v-2-lab-1-event-streams-create-topic-index-mdx",
    "path": "/event-automation/AsyncApi_v2/Lab1-EventStreams-Create-Topic/",
    "result": {"pageContext":{"frontmatter":{"description":"Event Automation / Async Api V 2 / Lab 1 Event Streams Create Topic","title":"Event Automation / Async Api V 2 / Lab 1 Event Streams Create Topic"},"relativePagePath":"/event-automation/AsyncApi_v2/Lab1-EventStreams-Create-Topic/index.mdx","titleType":"page","MdxNode":{"id":"6947b93b-0349-5f20-a55b-63dfffc0b4cc","children":[],"parent":"f050e830-65ce-550d-842d-3d37441ac52b","internal":{"content":"# IBM App Connect Enterprise\n\n## App Connect Kafka Designer Event flow\n\n[Return to main lab page](../index.md)\n\n---\n\n# Table of Contents \n- [1. Introduction](#introduction)\n- [2. Setup connection to Smart connectors for this lab](#Setup_connections)\n  * [2.1. Create the Kafka topic.](#Setup_kafka)\n\t* [2.2. Create MQ Queue for the consumer.](#Setup_MQ)\n- [3. Create API to publish message to topic and event flow to consume topics.](#Setup_API)\n  * [3.1. Create API to publish message to Kafka topic.](#Create_API)\n\t* [3.2. Create Event-driven flow to consume Kafka messages.](#Create_Consumer)\n- [4. Testing the Kafka flows](#test_designer_flow)\n- [5. Deploying Your Designer Flow to App Connect Dashboard  ](#deploy_a_designer_flow)\n    \n---\n\n# 1. Introduction <a name=\"introduction\"></a>\n\nReact to events in real time to deliver responsive and personalized experiences for applications and customer experience.  Built on open source Apache Kafka, IBM Event Streams is an event-streaming platform that helps you build smart applications that can react to events as they happen.\n\n* The purpose of this LAB is to show how to publish messages on to the Kafka broker to be consumed by other applications that are listening to a topic.  We will create a simple API that will create messages and then publish them to our topic.  We will also then create an event driven flow that will be listening to the topic and will put the message on to a MQ queue.\n\nIf you need to review logging in to the Platform Navigator review the steps in the [Return to main lab page](../../../index.md#lab-sections)\n\n1\\. From your home page in the upper left it will show your login name and you can always click on **IBM Automation** to get to this home page.   \n* For this lab we will be using **App Connect Designer**, under Integration Messaging we will use **MQ**, and under Event Streaming we will use **es-demo** for our Kafka broker.  \n\n![alt text][pic2]\n\n2\\. You can right click on each of these capabilities and open them in a new tab.  \nYou wil then have a tab open for each of them to help save time.  \n\n![alt text][pic3]\n\n# 2. Create the Kafka topic and MQ Queue for this lab.<a name=\"Setup_connections\"></a>\n# 2.1 Create the Kafka topic<a name=\"Setup_kafka\"></a>\n\n1\\. Now go to the tab for **es-demo** or you can click on the link from the home page and this will take you to the IBM Event Streams home page.   \n\n![alt text][es1]\n\n2\\. Now we will create our topic. Click on the **Create a topic** tile. \n\n![alt text][es2]\n\n3\\. Now enter the name of your topic.  Since this is a shared Kafka broker, use your userid as part of the topic name.  In this example, we are logged in as chopper1 so the topic name is **chopper1.mytopic**.  Click **Next**.\n\n![alt text][es3]\n\n4\\. Leave the partitions at 1 and click **Next**.\n\n![alt text][es4]\n\n5\\. Since this is for a lab, change the Message retention to **A day** and click **Next**.\n**Note:** If we wanted to retain message longer we could change this to meet those needs.\n\n![alt text][es5]\n\n6\\. For the Replicas, we will change the **Minimum in-sync replicas** to **1** and select **Replication factor:1**.  Note:  Make sure that the radial button next to Replication factor:1 is selected before clicking **Create topic**.\n\n![alt text][es6]\n\n7\\. Now you will be back on the Topics screen.  You may see other users topics in the list but make sure you see your topic you just created.  \n\n# 2.2 Create MQ Queue for the consumer<a name=\"Setup_MQ\"></a>\n\n1\\.Now go to the tab for **MQ Console** \n\n![alt text][pic3]\n\nOr you can click on the link from the home page and this will take you to the IBM MQ Console home page.   \n\n![alt text][mq1]\n\n2\\. Now click on your Queue Manger tile.  \n\n![alt text][mq2]\n\n3\\. The name of your Queue Manager will be MQGR**X** where **X** = the number of your userid.  In this example we are using chopper1.  Save this for later use.   \nWe will be creating a new local Queue for this lab.   Click on **Create**\n\n![alt text][mq3]\n\n4\\. Click on the local Queue tile.  \n\n![alt text][mq4]\n\n5\\. Now enter the Queue name.  In the example we used **DEMO.MYTOPIC.EVENT** then click **Create**.  \n\n![alt text][mq5]\n\n6\\. We now have your Queue defined.   Click on the **IBM Automation** on the upper left to go back to the homepage. \n\n![alt text][mq6]\n\n[pic0]: images/0.png\n[pic1]: images/1.png\n[pic2]: images/2.png\n[pic3]: images/3.png\n[es1]: images/es1.png\n[es2]: images/es2.png\n[es3]: images/es3.png\n[es4]: images/es4.png\n[es5]: images/es5.png\n[es6]: images/es6.png\n[mq1]: images/mq1.png\n[mq2]: images/mq2.png\n[mq3]: images/mq3.png\n[mq4]: images/mq4.png\n[mq5]: images/mq5.png\n[mq6]: images/mq6.png\n\n# 3. Create API to publish message to topic and event flow to consume topics.<a name=\"Setup_API\"></a>\n\nIn this section, we will create a simple API that will create messages and then publish them to our topic.  We will also then create an event driven flow that will be listening to the topic and will put the message on to a MQ queue.\n\n**Note:** We are just building the one flow to put the events to a queue but you could also build additional flows for other business units to react to the same message and send email, Slack, etc. \n\n# 3.1 Create API to publish message to Kafka topic<a name=\"Create_API\"></a>\n\n1\\.Now go to the tab for **IBM Automation** \n\n![alt text][pic3]\n\nOr you can click on the link from the home page and this will take you to the IBM ACE Designer page.   \n\n![alt text][des1]\n\n2\\. You should be on the App Connect Designer home page.   On the left menu select the Dashboard icon.  Once on the Dashboard page on the right side select *New** and click on **Flows for an API**.  \n\n![alt text][des2]\n\n3\\. First thing we will do is create the model for this.  We will call the model **KafkaProducer**\n\n![alt text][des3]\n\n4\\. For this example, we will map the following properties these will all be data type String except for amount we will change that to number. \n\n**Note:** The \"Add property +\" is used to add additional property fields. \n\n1. id\n2. name\n3. amount\n4. description\n\nWhen done click on the **Operations tab.**\n\n![alt text][des4]\n\n5\\. We will now select the operation we will build for this API.  \nClick on the drop down to add an operation. \n\n![alt text][des5]\n\n6\\. We will use the Create operation for this.  Click on that.\n\n![alt text][des6]\n\n7\\. We now have the POST operation ready and will implement our flow.   Click on the **Implement Flow** button.  This will take us to the flow designer.  \n\n![alt text][des7]\n\n8\\. Now click on the blue **+** and enter **Kafka** on the search line or scroll down to the **Kafka connector**.  Select the **Send message**. \n\n![alt text][des8]\n\n9\\.If you already have a connection you can skip to **Step 12**\nIf not, click on **Connect**.\n\n![alt text][des9]\n\n10\\. For the Authorization method, make sure to select **\\(SASL_SSL\\)** from the dropdown.  Click **Continue**. \n\n![alt text][des10]\n\n11\\. Now fill in the fields.  We will use the SCRAM credentials we saved earlier in the **Kafka Pre-Req**. \n<br/>[Return to main lab page and goto the **Create Connection to shared Kafka cluster**](../../../index.md#lab-sections)\n  \n* a\\. Kafka broker list: bootstrap URL of the cluster\n* b\\. username: SCRAM username \n* c\\. password: SCRAM password \n\n* For the Security mechaniam: make sure to select the **512**\n* Open the es-cert.pem file we downloaded.  Copy the whole thing and paste in the CA certiticate.  Scroll to the bottom and Click **Connect**.\n\n![alt text][des11]\n\n12\\. Now select the topic for your userid that you created in the last section.  For the payload, we will fill it in with the mapping the input to the API.  \n\n![alt text][des12]\n\n13\\. We will complete the API by updating the Reponse.  Click on the Reponse.\nWe will map the Kafka Offset to the id in the reponse of the API.  \nWhen done click on the **Done** button in the upper right.  \n\n![alt text][des13]\n\n[des1]: images/des1.png\n[des2]: images/des2.png\n[des3]: images/des3.png\n[des4]: images/des4.png\n[des5]: images/des5.png\n[des6]: images/des6.png\n[des7]: images/des7.png\n[des8]: images/des8.png\n[des9]: images/des9.png\n[des10]: images/des10.png\n[des11]: images/des11.png\n[des12]: images/des12.png\n[des13]: images/des13.png\n\n# 3.2 Create Event-driven flow to consume Kafka messages.<a name=\"Create_Consumer\"></a>\n\n1\\. Now go to the App Connect Designer Dashboard by clicking on the left hand menu icon\n\n![alt text][des14]\n\n2\\. You will now see the API flow you just created for producing Kafka messages to your topic. \nNext, we will click on **New** - **Event-driven flow** to create the consumer flow.\n\n![alt text][des15]\n\n3\\. Now click on the blue **+** and scroll down to the **Kafka** connector or just start typing in the search field.  Select the **New message**.\nYou should already have an Account setup from the last section.   \n\n![alt text][des16]\n\n4\\. Now we will see the configuration screen for the Kafka connector. \n Now select the topic for your userid that you created in the previous section.  For the Group ID, we will use your userid as the unique ID.  In this case, we are using chopper1.\n\n **Note:** Make sure you use your userid for this. \n\n **Note:** For the **Message offset** you can select the latest which will start to consume messages at that point.   If you select eariler then you will get all messages that had been produced already for the offset.\n \n **Do not leave Message offset blank**\n\n![alt text][des17]\n\n5\\. Now click on the blue **+** and scroll down to the **IBM MQ** connector or just start typing in the search field.  Select the **Put message on a queue**. \n\n![alt text][des18]\n\n6\\. Select the **Put message on a queue**. If you don't have an Account already setup for your MQ connector **click on Connect**\nIf you have a Account already setup skip to **Step XX**\n\n![alt text][des19]\n\n7\\. Now you will fill in the connection details from the MQ-Pre-Lab where you should have saved your **Queue Manager Name** and **Hostname**.\n  \n* Enter the QMgr name\n* For the QMgr host we will use the following format:\n''''\nqmgrxx-ibm-mq.\\<userid\\>.svc\n\n  xx = userid number\n* Port is 1414\n* Channel SYSTEM.DEF.SVRCONN\n''''\n\nClick **Connect**\n\n![alt text][des20]\n\n7\\. Click **Continue**\n\n![alt text][des21]\n\n8\\. Now we will complete the mapping for our MQ connector.  \n\n1. Queue name:      DEMO.MYTOPIC.EVENT\n2. Message type:    TEXT \n3. Message payload: If you click in the box the suggested mapping is displayed.  Select the **Payload**. \n\nWe will also give the flow a meaningful name.   In this example we can use **Consume Kafka messages**.  \nWhen done click on the **Dashboard** in the upper left corner. \n\n![alt text][des22]\n\n9\\. Now from the Dashboard we see our two flows we created.  Now continue to the next section to test the flows.   \n\n![alt text][des23]\n\n[des14]: images/des14.png\n[des15]: images/des15.png\n[des16]: images/des16.png\n[des17]: images/des17.png\n[des18]: images/des18.png\n[des19]: images/des19.png\n[des20]: images/des20.png\n[des21]: images/des21.png\n[des22]: images/des22.png\n[des23]: images/des23.png\n\n\n# 4 Testing the Kafka flows <a name=\"test_designer_flow\"></a>\n\nWe will now test the new Kafka flows.   \n\n1\\. You will now be on your home page and in the upper left it will show your login name and you can always click on **IBM Automation** to get to this home page.   \n* For this lab we are using **App Connect Designer**, under Integration Messaging we will use **MQ**, and under Event Streaming we will use **es-demo** for our Kafka broker.  \n\n![alt text][pic2]\n\n2\\. To make testing easier you should right click on each of these capiblities and open them in a new tab.  \nYou will then have a tab open for each of them to help save time.  \n\n![alt text][pic3]\n\n3\\. Let's first go to the App Connect Dashboard.  Here you will see your Kafka flows created.  We will start with the **Kafka Producer API** click on the tile to open it. \n\n![alt text][tst1]\n\n4\\. Now in the upper right corner we will click on the switch to start the API. \n\n![alt text][tst2]\n\n5\\. You will now see that the Test button is on the menu.   Click on the **Test Button** and you will see the API test page.   Click on the POST operation.  \n\n![alt text][tst3]\n\n6\\. Next we will click on the **Try it** button.  \n\n![alt text][tst4]\n\n7\\. Now scroll down the API test page and you can click on **Generate** to populate the body of the API.  This will show all the fields for the API call.  You can change the fields if you like as in this example.\nClick **Send** button. \n\nYOu will then see the API Request and the Reponse to the API call.   We have just produced a kafka message and the offset is 5 which is returned in the API call. \n\n![alt text][tst5]\n\n8\\. Now let's go to the Event Streams tab and click on the left menu to open the topics.  You should see your topic that you created in the first section of this lab.  In this example it is chopper1.mytopic. \n\n![alt text][tst6]\n\n9\\. Click on your topic to open your topic.  On your topic page click on the Message button and this will show the message you just created. \n\n![alt text][tst7]\n\nYou can click on the message to open it and see the content. \n\n![alt text][tst7a]\n\n10\\. Now let's go to the MQ Console tab and click on your Queue Manager title, in this example it is MQGR1. \nYou will see that your queue should show zero messages.  This is since we didn't start the consumer flow yet to put the kafka messages to the queue.  \n\n![alt text][tst8]\n\n11\\. Now let's go back to the to the  App Connect Dashboard.  You will see that the Consumer flow is not running.  Click on the 3 dots of the tile and select start to start the flow.   \n\n![alt text][tst9]\n\n12\\. You now should see the consumer flow Running. \n\n![alt text][tst10]\n\n13\\. Now let's go back to the MQ Console tab and click on your Queue Manager title, in this example it is MQGR1. \nClick on the Refresh Icon and you should see a message on your queue now.  **Note** This is due to Kafka broker keeping the messages available so when applications start up they can go back and pick up messages that have already been produced.   \n\n![alt text][tst11]\n\n14\\.You can click on the queue to view the message and the data. \n\n![alt text][tst12]\n\n[tst1]: images/tst1.png\n[tst2]: images/tst2.png\n[tst3]: images/tst3.png\n[tst4]: images/tst4.png\n[tst5]: images/tst5.png\n[tst6]: images/tst6.png\n[tst7]: images/tst7.png\n[tst7a]: images/tst7a.png\n[tst8]: images/tst8.png\n[tst9]: images/tst9.png\n[tst10]: images/tst10.png\n[tst11]: images/tst11.png\n[tst12]: images/tst12.png\n\n\n## Summary\nYou can go back and produce more messages using the API flow and stop and start the consumer flow as well. \n\n# 5. Deploying Your Designer Flow to App Connect Dashboard <a name=\"deploy_a_designer_flow\"></a>\n\nAs in other labs, we can export our Designer flow as a bar file and deploy to App Connect Dashboard on Cloud Pak for Integration. We will not do that in this lab.   \n\n\n[Return to main lab page](../index.md)\n","type":"Mdx","contentDigest":"19fb5460fe30daf2a0dc7c2b6e80d4ef","owner":"gatsby-plugin-mdx","counter":2650},"frontmatter":{"description":"Event Automation / Async Api V 2 / Lab 1 Event Streams Create Topic","title":"Event Automation / Async Api V 2 / Lab 1 Event Streams Create Topic"},"exports":{},"rawBody":"# IBM App Connect Enterprise\n\n## App Connect Kafka Designer Event flow\n\n[Return to main lab page](../index.md)\n\n---\n\n# Table of Contents \n- [1. Introduction](#introduction)\n- [2. Setup connection to Smart connectors for this lab](#Setup_connections)\n  * [2.1. Create the Kafka topic.](#Setup_kafka)\n\t* [2.2. Create MQ Queue for the consumer.](#Setup_MQ)\n- [3. Create API to publish message to topic and event flow to consume topics.](#Setup_API)\n  * [3.1. Create API to publish message to Kafka topic.](#Create_API)\n\t* [3.2. Create Event-driven flow to consume Kafka messages.](#Create_Consumer)\n- [4. Testing the Kafka flows](#test_designer_flow)\n- [5. Deploying Your Designer Flow to App Connect Dashboard  ](#deploy_a_designer_flow)\n    \n---\n\n# 1. Introduction <a name=\"introduction\"></a>\n\nReact to events in real time to deliver responsive and personalized experiences for applications and customer experience.  Built on open source Apache Kafka, IBM Event Streams is an event-streaming platform that helps you build smart applications that can react to events as they happen.\n\n* The purpose of this LAB is to show how to publish messages on to the Kafka broker to be consumed by other applications that are listening to a topic.  We will create a simple API that will create messages and then publish them to our topic.  We will also then create an event driven flow that will be listening to the topic and will put the message on to a MQ queue.\n\nIf you need to review logging in to the Platform Navigator review the steps in the [Return to main lab page](../../../index.md#lab-sections)\n\n1\\. From your home page in the upper left it will show your login name and you can always click on **IBM Automation** to get to this home page.   \n* For this lab we will be using **App Connect Designer**, under Integration Messaging we will use **MQ**, and under Event Streaming we will use **es-demo** for our Kafka broker.  \n\n![alt text][pic2]\n\n2\\. You can right click on each of these capabilities and open them in a new tab.  \nYou wil then have a tab open for each of them to help save time.  \n\n![alt text][pic3]\n\n# 2. Create the Kafka topic and MQ Queue for this lab.<a name=\"Setup_connections\"></a>\n# 2.1 Create the Kafka topic<a name=\"Setup_kafka\"></a>\n\n1\\. Now go to the tab for **es-demo** or you can click on the link from the home page and this will take you to the IBM Event Streams home page.   \n\n![alt text][es1]\n\n2\\. Now we will create our topic. Click on the **Create a topic** tile. \n\n![alt text][es2]\n\n3\\. Now enter the name of your topic.  Since this is a shared Kafka broker, use your userid as part of the topic name.  In this example, we are logged in as chopper1 so the topic name is **chopper1.mytopic**.  Click **Next**.\n\n![alt text][es3]\n\n4\\. Leave the partitions at 1 and click **Next**.\n\n![alt text][es4]\n\n5\\. Since this is for a lab, change the Message retention to **A day** and click **Next**.\n**Note:** If we wanted to retain message longer we could change this to meet those needs.\n\n![alt text][es5]\n\n6\\. For the Replicas, we will change the **Minimum in-sync replicas** to **1** and select **Replication factor:1**.  Note:  Make sure that the radial button next to Replication factor:1 is selected before clicking **Create topic**.\n\n![alt text][es6]\n\n7\\. Now you will be back on the Topics screen.  You may see other users topics in the list but make sure you see your topic you just created.  \n\n# 2.2 Create MQ Queue for the consumer<a name=\"Setup_MQ\"></a>\n\n1\\.Now go to the tab for **MQ Console** \n\n![alt text][pic3]\n\nOr you can click on the link from the home page and this will take you to the IBM MQ Console home page.   \n\n![alt text][mq1]\n\n2\\. Now click on your Queue Manger tile.  \n\n![alt text][mq2]\n\n3\\. The name of your Queue Manager will be MQGR**X** where **X** = the number of your userid.  In this example we are using chopper1.  Save this for later use.   \nWe will be creating a new local Queue for this lab.   Click on **Create**\n\n![alt text][mq3]\n\n4\\. Click on the local Queue tile.  \n\n![alt text][mq4]\n\n5\\. Now enter the Queue name.  In the example we used **DEMO.MYTOPIC.EVENT** then click **Create**.  \n\n![alt text][mq5]\n\n6\\. We now have your Queue defined.   Click on the **IBM Automation** on the upper left to go back to the homepage. \n\n![alt text][mq6]\n\n[pic0]: images/0.png\n[pic1]: images/1.png\n[pic2]: images/2.png\n[pic3]: images/3.png\n[es1]: images/es1.png\n[es2]: images/es2.png\n[es3]: images/es3.png\n[es4]: images/es4.png\n[es5]: images/es5.png\n[es6]: images/es6.png\n[mq1]: images/mq1.png\n[mq2]: images/mq2.png\n[mq3]: images/mq3.png\n[mq4]: images/mq4.png\n[mq5]: images/mq5.png\n[mq6]: images/mq6.png\n\n# 3. Create API to publish message to topic and event flow to consume topics.<a name=\"Setup_API\"></a>\n\nIn this section, we will create a simple API that will create messages and then publish them to our topic.  We will also then create an event driven flow that will be listening to the topic and will put the message on to a MQ queue.\n\n**Note:** We are just building the one flow to put the events to a queue but you could also build additional flows for other business units to react to the same message and send email, Slack, etc. \n\n# 3.1 Create API to publish message to Kafka topic<a name=\"Create_API\"></a>\n\n1\\.Now go to the tab for **IBM Automation** \n\n![alt text][pic3]\n\nOr you can click on the link from the home page and this will take you to the IBM ACE Designer page.   \n\n![alt text][des1]\n\n2\\. You should be on the App Connect Designer home page.   On the left menu select the Dashboard icon.  Once on the Dashboard page on the right side select *New** and click on **Flows for an API**.  \n\n![alt text][des2]\n\n3\\. First thing we will do is create the model for this.  We will call the model **KafkaProducer**\n\n![alt text][des3]\n\n4\\. For this example, we will map the following properties these will all be data type String except for amount we will change that to number. \n\n**Note:** The \"Add property +\" is used to add additional property fields. \n\n1. id\n2. name\n3. amount\n4. description\n\nWhen done click on the **Operations tab.**\n\n![alt text][des4]\n\n5\\. We will now select the operation we will build for this API.  \nClick on the drop down to add an operation. \n\n![alt text][des5]\n\n6\\. We will use the Create operation for this.  Click on that.\n\n![alt text][des6]\n\n7\\. We now have the POST operation ready and will implement our flow.   Click on the **Implement Flow** button.  This will take us to the flow designer.  \n\n![alt text][des7]\n\n8\\. Now click on the blue **+** and enter **Kafka** on the search line or scroll down to the **Kafka connector**.  Select the **Send message**. \n\n![alt text][des8]\n\n9\\.If you already have a connection you can skip to **Step 12**\nIf not, click on **Connect**.\n\n![alt text][des9]\n\n10\\. For the Authorization method, make sure to select **\\(SASL_SSL\\)** from the dropdown.  Click **Continue**. \n\n![alt text][des10]\n\n11\\. Now fill in the fields.  We will use the SCRAM credentials we saved earlier in the **Kafka Pre-Req**. \n<br/>[Return to main lab page and goto the **Create Connection to shared Kafka cluster**](../../../index.md#lab-sections)\n  \n* a\\. Kafka broker list: bootstrap URL of the cluster\n* b\\. username: SCRAM username \n* c\\. password: SCRAM password \n\n* For the Security mechaniam: make sure to select the **512**\n* Open the es-cert.pem file we downloaded.  Copy the whole thing and paste in the CA certiticate.  Scroll to the bottom and Click **Connect**.\n\n![alt text][des11]\n\n12\\. Now select the topic for your userid that you created in the last section.  For the payload, we will fill it in with the mapping the input to the API.  \n\n![alt text][des12]\n\n13\\. We will complete the API by updating the Reponse.  Click on the Reponse.\nWe will map the Kafka Offset to the id in the reponse of the API.  \nWhen done click on the **Done** button in the upper right.  \n\n![alt text][des13]\n\n[des1]: images/des1.png\n[des2]: images/des2.png\n[des3]: images/des3.png\n[des4]: images/des4.png\n[des5]: images/des5.png\n[des6]: images/des6.png\n[des7]: images/des7.png\n[des8]: images/des8.png\n[des9]: images/des9.png\n[des10]: images/des10.png\n[des11]: images/des11.png\n[des12]: images/des12.png\n[des13]: images/des13.png\n\n# 3.2 Create Event-driven flow to consume Kafka messages.<a name=\"Create_Consumer\"></a>\n\n1\\. Now go to the App Connect Designer Dashboard by clicking on the left hand menu icon\n\n![alt text][des14]\n\n2\\. You will now see the API flow you just created for producing Kafka messages to your topic. \nNext, we will click on **New** - **Event-driven flow** to create the consumer flow.\n\n![alt text][des15]\n\n3\\. Now click on the blue **+** and scroll down to the **Kafka** connector or just start typing in the search field.  Select the **New message**.\nYou should already have an Account setup from the last section.   \n\n![alt text][des16]\n\n4\\. Now we will see the configuration screen for the Kafka connector. \n Now select the topic for your userid that you created in the previous section.  For the Group ID, we will use your userid as the unique ID.  In this case, we are using chopper1.\n\n **Note:** Make sure you use your userid for this. \n\n **Note:** For the **Message offset** you can select the latest which will start to consume messages at that point.   If you select eariler then you will get all messages that had been produced already for the offset.\n \n **Do not leave Message offset blank**\n\n![alt text][des17]\n\n5\\. Now click on the blue **+** and scroll down to the **IBM MQ** connector or just start typing in the search field.  Select the **Put message on a queue**. \n\n![alt text][des18]\n\n6\\. Select the **Put message on a queue**. If you don't have an Account already setup for your MQ connector **click on Connect**\nIf you have a Account already setup skip to **Step XX**\n\n![alt text][des19]\n\n7\\. Now you will fill in the connection details from the MQ-Pre-Lab where you should have saved your **Queue Manager Name** and **Hostname**.\n  \n* Enter the QMgr name\n* For the QMgr host we will use the following format:\n''''\nqmgrxx-ibm-mq.\\<userid\\>.svc\n\n  xx = userid number\n* Port is 1414\n* Channel SYSTEM.DEF.SVRCONN\n''''\n\nClick **Connect**\n\n![alt text][des20]\n\n7\\. Click **Continue**\n\n![alt text][des21]\n\n8\\. Now we will complete the mapping for our MQ connector.  \n\n1. Queue name:      DEMO.MYTOPIC.EVENT\n2. Message type:    TEXT \n3. Message payload: If you click in the box the suggested mapping is displayed.  Select the **Payload**. \n\nWe will also give the flow a meaningful name.   In this example we can use **Consume Kafka messages**.  \nWhen done click on the **Dashboard** in the upper left corner. \n\n![alt text][des22]\n\n9\\. Now from the Dashboard we see our two flows we created.  Now continue to the next section to test the flows.   \n\n![alt text][des23]\n\n[des14]: images/des14.png\n[des15]: images/des15.png\n[des16]: images/des16.png\n[des17]: images/des17.png\n[des18]: images/des18.png\n[des19]: images/des19.png\n[des20]: images/des20.png\n[des21]: images/des21.png\n[des22]: images/des22.png\n[des23]: images/des23.png\n\n\n# 4 Testing the Kafka flows <a name=\"test_designer_flow\"></a>\n\nWe will now test the new Kafka flows.   \n\n1\\. You will now be on your home page and in the upper left it will show your login name and you can always click on **IBM Automation** to get to this home page.   \n* For this lab we are using **App Connect Designer**, under Integration Messaging we will use **MQ**, and under Event Streaming we will use **es-demo** for our Kafka broker.  \n\n![alt text][pic2]\n\n2\\. To make testing easier you should right click on each of these capiblities and open them in a new tab.  \nYou will then have a tab open for each of them to help save time.  \n\n![alt text][pic3]\n\n3\\. Let's first go to the App Connect Dashboard.  Here you will see your Kafka flows created.  We will start with the **Kafka Producer API** click on the tile to open it. \n\n![alt text][tst1]\n\n4\\. Now in the upper right corner we will click on the switch to start the API. \n\n![alt text][tst2]\n\n5\\. You will now see that the Test button is on the menu.   Click on the **Test Button** and you will see the API test page.   Click on the POST operation.  \n\n![alt text][tst3]\n\n6\\. Next we will click on the **Try it** button.  \n\n![alt text][tst4]\n\n7\\. Now scroll down the API test page and you can click on **Generate** to populate the body of the API.  This will show all the fields for the API call.  You can change the fields if you like as in this example.\nClick **Send** button. \n\nYOu will then see the API Request and the Reponse to the API call.   We have just produced a kafka message and the offset is 5 which is returned in the API call. \n\n![alt text][tst5]\n\n8\\. Now let's go to the Event Streams tab and click on the left menu to open the topics.  You should see your topic that you created in the first section of this lab.  In this example it is chopper1.mytopic. \n\n![alt text][tst6]\n\n9\\. Click on your topic to open your topic.  On your topic page click on the Message button and this will show the message you just created. \n\n![alt text][tst7]\n\nYou can click on the message to open it and see the content. \n\n![alt text][tst7a]\n\n10\\. Now let's go to the MQ Console tab and click on your Queue Manager title, in this example it is MQGR1. \nYou will see that your queue should show zero messages.  This is since we didn't start the consumer flow yet to put the kafka messages to the queue.  \n\n![alt text][tst8]\n\n11\\. Now let's go back to the to the  App Connect Dashboard.  You will see that the Consumer flow is not running.  Click on the 3 dots of the tile and select start to start the flow.   \n\n![alt text][tst9]\n\n12\\. You now should see the consumer flow Running. \n\n![alt text][tst10]\n\n13\\. Now let's go back to the MQ Console tab and click on your Queue Manager title, in this example it is MQGR1. \nClick on the Refresh Icon and you should see a message on your queue now.  **Note** This is due to Kafka broker keeping the messages available so when applications start up they can go back and pick up messages that have already been produced.   \n\n![alt text][tst11]\n\n14\\.You can click on the queue to view the message and the data. \n\n![alt text][tst12]\n\n[tst1]: images/tst1.png\n[tst2]: images/tst2.png\n[tst3]: images/tst3.png\n[tst4]: images/tst4.png\n[tst5]: images/tst5.png\n[tst6]: images/tst6.png\n[tst7]: images/tst7.png\n[tst7a]: images/tst7a.png\n[tst8]: images/tst8.png\n[tst9]: images/tst9.png\n[tst10]: images/tst10.png\n[tst11]: images/tst11.png\n[tst12]: images/tst12.png\n\n\n## Summary\nYou can go back and produce more messages using the API flow and stop and start the consumer flow as well. \n\n# 5. Deploying Your Designer Flow to App Connect Dashboard <a name=\"deploy_a_designer_flow\"></a>\n\nAs in other labs, we can export our Designer flow as a bar file and deploy to App Connect Dashboard on Cloud Pak for Integration. We will not do that in this lab.   \n\n\n[Return to main lab page](../index.md)\n","fileAbsolutePath":"/Users/luciano/pot-workspace/my-carbon-site/src/pages/event-automation/AsyncApi_v2/Lab1-EventStreams-Create-Topic/index.mdx"}}},
    "staticQueryHashes": ["1364590287","137577622","2102389209","2456312558","2746626797","3018647132","3037994772","768070550"]}